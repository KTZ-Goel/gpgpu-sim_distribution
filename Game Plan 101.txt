Simply Optmistic changes: 

0. Create data structure where gpu address is mapped to its corresponding page data structure, (this should be done at cudaMallocManaged) : Here, gpu address 
   and the size will be used to fill corresponding pages and its page related data structure.  
    De Dana DONE

1. We have to make two Queues: -> One from CU To GMMU  and One from GMMU to CU, inspired by Ganguly (Dada).  

2. In Shader.cc: -> Create an function, (kind of like access_cycle): Get the page Id from the input address and get whether 
it is managed or not. If not valid (then page fault), push the instruction to cluster units to gmmu queue then delete the instruction from the instruction queue (accessq_pop).
  
   - 2.1 : Make pages managed in cudaMallocManaged and valid in cUDAMemPrefetchAsync: DONE (MALLOC MANAGED PART)

3. make changes in Shader.cc/memory_cycle function: 
   >>  Add an else condition if inst.access_queue is empty, remove the assert.
   >>  If empty (else part), pop the instruction from gmmu to cluster unit queue, and check if any instrcution available.  
   >>  If available, then do the same things that has been done, except for this instruction that is popped. 
    
*****************
cu_gmmu_queue -> for cluster unit to memory unit for gpu
    handles request R/W access requests for Core Clusters and sends to gmmu queue
gmmu_cu_queue -> for gpu memory unit to cluster units
    handles all fetched accesses over pcie to complete the requests

ldst_unit :: mcore[]->c_cu_queue
    handles access requests from cores to cluster units
ldst_unit :: mcore[]->cu_c_queue
    handles fecthed requests back to cores
******************
DONE

4. Make sure to write some algo, where we take from cores(c_cu and cu_c) to a Cluster Unit queues(cu_gmmu and gmmu_cu) and visa versa.   DONE

5. Make you own cycle function: GMMU_CYLE : 
    check whether there is any request in CU_GMMU queue,
        if yes, then check what type of latency it is.
            if its a page fault then do not service it until it is resolved to pcie read
                once the latency is resolved push the command to gmmu_cu queue, and then pop it from cu_gmmu queue.

    5.1. first compute all the page faults 

DONE!


5.2 Add latency in cluster::ICNT_cycle and pop/push into the cu_gmmu/gmmu_cu queues... -  DOne and Working

6. Work on adding latency in gpu-sim.cc though a memunit_cycle... - Done


Latency Assumptions : Readme
Currently, there is no delay for page table lookup. If there is a miss, then the combined delay is incurred by the page fault.
This is because of the lack of TLB.. By assuming TLB hit as equal to page table hit. 

--done >> Step 0: in the accessq_cycle: make sure to put the request even if one its pages from start to end, is managed and invalid.
--done >> Make a new queue: page_fault_latency
>> Extract a new memory request from m_cu_gmmu_queue and get its first page and last page-> Push this into latency_queue.
   Extract a request from m_cu_gmmu_queue and forthat request Traverse for each page, and check for two things: { already present}
                                                    i) Is the page already valid (get_faulty_pages) { No need for this step, just iterate through faulty pages}
                                                    ii) Is the page already in the list of page_fault_queue (For this traverse the page and check whether it matches any of the pages)
   If that page returns false, for both of above, then it is a new request and put that request in the page_fault_queue list.
   Pop the front of m_cu_gmmu_queue. 
--done >> Now, check from page_fault_queue list, each page, whether it is ready or not, if ready pop that page from the queue, and mark that page as valid.
--done >> Now, for each memory request in the latency_queue, check whether all of its pages are valid, if all are valid, then pop it from latency_queue and push it to the
   gmmu_cu queue


Side Algo:
>> Step 0: in the accessq_cycle: make sure to put the request even if one its pages from start to end, is managed and invalid.
>> Extract a new memory request from m_cu_gmmu_queue and get its first page and last page-> Push this into latency_queue and pop it from cu_gmmu_q
>> Traverse for each page from latency_queue and check whether 
                                

Wildly Optimistic Future : 
    Prefetching can be added. 

Adding CudaMemPrefetchAsync: 

1. Make a function, to be added in the stream_manager.cc/g_stream....
2. Make structure to hold individual prefetch requests, same as dada in gpu_sim class.
3. CudaMemPrefetchAsync calls the function and through stream manager the gpgpu-sim function is called...
    3.1. This function pushes to the prefetch_queue
4. Activate the request in do_operation from the stream_manager (checksif the prefetch request belongs to the particular stream and schedules it)
Done till 4

5. activate prefetch called without stream access for now       Done 
6. Prefetch working

7. TLB added
8. Living in 2050 now : Adding Page Eviction for Page Table and Page size

    To Be done :  add TLB_shootdown
                    shader.cc tlb and refresh
                    check existin pages in write queue and bring back if ready 0

------------- Added Page Eviction based on LRU policy
All done for now

-------------------------------------- Added Random HW Prefetcher ( fetches a page at a random distance (mod 5) from a faulty page)
Getting better IPC when runnign with than without.... Not much effect if Cuda Mem Prefetch is called earlier




STats

TLB -> Disable TLB (size =0) with and without
    - IPC
    - number of page faults

SW prefetching -> without CudaMemPrefetchAsync
    - IPC
    - Number of page-faults

HW Prefetching -> without cudaMemprefetch
	- show ipc difference with and without hw prefetcher
	- show number of observed page faults in case of page faults
